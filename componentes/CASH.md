## Seleção de modelos e Otimização de hiperparâmetros

**Trabalhos revisados:**


| Título | Chave bibtex | Ano | Link | Github | Página do resumo |
|:------:|:-------:|:---:|:----:|:----------------:|:----------------:|
|Hyperparameter Optimization|Feurer_2019b|2019|[aqui](https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1)|-|C1/26|
|Algorithms for hyper-parameter optimization|Bergstra_2011|2011|[aqui](https://dl.acm.org/doi/10.5555/2986459.2986743)|-||
|Meta-Learning|Vanschoren_2019|2019|[aqui](https://link.springer.com/chapter/10.1007/978-3-030-05318-5_2)|-|C1/32|
|Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets|FABOLAS_2017|2017|[aqui](https://arxiv.org/abs/1605.07079)|[aqui](https://github.com/automl/RoBO)|C1/73|
|BOHB: Robust and Efficient Hyperparameter Optimization at Scale|BOHB_2018|2018|[aqui](https://arxiv.org/abs/1807.01774)|[aqui](https://automl.github.io/HpBandSter/build/html/optimizers/bohb.html)|C1/73|
|Non-stochastic Best Arm Identification and Hyperparameter Optimization|Jamieson_2015|2015|[aqui](https://arxiv.org/abs/1502.07943)|||
|Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization|Hyperband_2016|2016|[aqui](https://arxiv.org/abs/1603.06560)|||
|Easy Hyperparameter Search Using Optunity|Claesen_2014|2014|[aqui](https://arxiv.org/abs/1412.1114)|||
|Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA|Auto-WEKA_2017|2017|[aqui](https://dl.acm.org/doi/10.5555/3122009.3122034)|[aqui](https://github.com/automl/autoweka)||
|Sequential Model-Based Optimization for General Algorithm Configuration|SMAC_2011|2011|[aqui](https://link.springer.com/chapter/10.1007/978-3-642-25566-3_40)|[aqui](https://github.com/automl/SMAC3)||
|RoBO: A Flexible and Robust Bayesian Optimization Framework in Python|RoBO_2017|2017|[aqui](https://tr.informatik.uni-freiburg.de/reports/report292/report00292.pdf)|[aqui](https://github.com/automl/RoBO)||
|Bayesian tuning and bandits : an extensible, open source library for AutoML|BTB_2018|2018|[aqui](https://dspace.mit.edu/handle/1721.1/119764)|[aqui](https://pypi.org/project/baytune/0.3.4/)||
|Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms|Hyperopt_2013|2013|[aqui](https://pdfs.semanticscholar.org/d4f4/9717c9adb46137f49606ebbdf17e3598b5a5.pdf)|[aqui](https://github.com/hyperopt/hyperopt)||
|Simple And Efficient Architecture Search for Convolutional Neural Networks|Elsken_2017|2017|[aqui](https://arxiv.org/abs/1711.04528)|||
|Neural Architecture Search with Reinforcement Learning|Zoph_2016|2016|[aqui](https://arxiv.org/abs/1611.01578)|||
|Gradient-based hyperparameter optimization through reversible learning|Maclaurin_2015|2015|[aqui](https://dl.acm.org/doi/10.5555/3045118.3045343)|||
|Automating Predictive Modeling Process using Reinforcement Learning|Khurana_2019|2019|[aqui](http://arxiv.org/abs/1903.00743)|||
|Autostacker: A Compositional Evolutionary Learning System|Autostacker_2018|2018|[aqui](https://arxiv.org/abs/1803.00684)|||
|DarwinML: A Graph-based Evolutionary Algorithm for Automated Machine Learning|DarwinML_2019|2019|[aqui](https://arxiv.org/abs/1901.08013)|||
|Benchmarking of hyperparameter optimization techniques for machine learning applications in production|Motz_2022|2022|[aqui](https://www.sciencedirect.com/science/article/pii/S2666912922000265)||C1/43|
|A tutorial on automatic hyperparameter tuning of deep spectral modelling for regression and classification tasks|Passos_2022|2022|[aqui](https://www.sciencedirect.com/science/article/pii/S0169743922000314)||C1/43|
